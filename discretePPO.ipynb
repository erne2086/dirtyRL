{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda43a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "env_name = \"LunarLander-v3\"\n",
    "env = gym.make(env_name, render_mode=\"human\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46f7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_envs = gym.make_vec(env_name, num_envs=2)\n",
    "state = vec_envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77fbfbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the environment in the notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "# Create environment with rgb_array for visualization (no video recording)\n",
    "env = gym.make(env_name, render_mode='rgb_array')\n",
    "print(\"Environment created:\", env)\n",
    "\n",
    "# Reset and get initial observation\n",
    "obs, info = env.reset()\n",
    "print(\"Observation space:\", obs.shape)\n",
    "print(\"Info:\", info)\n",
    "\n",
    "# Run a random episode and display it in real-time\n",
    "done = False\n",
    "start_time = time.time()\n",
    "\n",
    "# Create figure once\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "while time.time() - start_time < 10:  # Run for 10 seconds\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Get frame and display it\n",
    "    frame = env.render()\n",
    "    \n",
    "    # Clear previous plot and show new frame\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Pendulum Environment - Reward: {reward:.2f}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Reset if episode ends\n",
    "    if done or truncated:\n",
    "        obs, info = env.reset()\n",
    "    \n",
    "    time.sleep(0.05)  # Small delay to see the animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf25b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() # Sample a random action, the output is a np.int64 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d755c8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_envs.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e923629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "from typing import List, Tuple\n",
    "\n",
    "def initialize_weights(layer: nn.Module):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.xavier_uniform_(layer.weight)\n",
    "        if layer.bias is not None:\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = 128):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Softmax(dim=-1)  # For discrete action spaces\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.apply(initialize_weights)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        action_probs = self.actor(x)\n",
    "        state_value = self.critic(x)\n",
    "        return action_probs, state_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74ee39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import Tensor\n",
    "def compute_gae(rewards: List[np.ndarray], values: List[Tensor], next_value: Tensor, gamma: float = 0.99, lam: float = 0.95) -> List[Tensor]:\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * next_value - values[step]\n",
    "        gae = delta + gamma * lam * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "        next_value = values[step]\n",
    "    return returns\n",
    "\n",
    "def ppo_update(\n",
    "    actor_critic: ActorCritic,\n",
    "    optimizer: Adam,\n",
    "    states: torch.Tensor,\n",
    "    actions: torch.Tensor,\n",
    "    old_log_probs: torch.Tensor,\n",
    "    returns: torch.Tensor,\n",
    "    advantages: torch.Tensor,\n",
    "    clip_ratio: float = 0.2\n",
    "):\n",
    "    action_probs, state_values = actor_critic(states)\n",
    "    \n",
    "    # Calculate log probabilities\n",
    "    dist = Categorical(action_probs)\n",
    "    log_probs = dist.log_prob(actions)\n",
    "    \n",
    "    # Calculate ratios\n",
    "    ratios = torch.exp(log_probs - old_log_probs)\n",
    "    \n",
    "    # Calculate surrogate losses\n",
    "    surrogate1 = ratios * advantages\n",
    "    surrogate2 = torch.clamp(ratios, 1 - clip_ratio, 1 + clip_ratio) * advantages\n",
    "    \n",
    "    actor_loss = -torch.min(surrogate1, surrogate2).mean() - 0.01 * dist.entropy().mean()  # Add entropy bonus for exploration\n",
    "    critic_loss = nn.MSELoss()(state_values.squeeze(), returns)\n",
    "    \n",
    "    loss = actor_loss + critic_loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c7b4642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_envs.action_space[0].n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0dcc4d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_envs.observation_space.shape \n",
    "# its 2 environments, with 8 observation elements each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce57c5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 3e-4\n",
    "input_dim = vec_envs.observation_space.shape[1]\n",
    "output_dim = int(vec_envs.action_space[0].n) # For discrete action spaces, it is one per keyboard in the discrete action space\n",
    "hidden_dim = 512\n",
    "num_episodes = 10_000\n",
    "max_frames = 30_000\n",
    "\n",
    "# Initialize actor-critic model and optimizer\n",
    "actor_critic = ActorCritic(input_dim, output_dim, hidden_dim)\n",
    "optimizer = Adam(actor_critic.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29daa644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 4)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48ab4ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def plot_rewards(rewards: List[float], title: str = \"Training Rewards\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards, label='Average Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_env(env, actor_critic, num_episodes: int = 10, visualize: bool = True):\n",
    "    test_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs, _ = actor_critic(state_tensor)\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            next_state, reward, done, truncated, info = env.step(action.item())\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if visualize:\n",
    "                frame = env.render()\n",
    "                plt.imshow(frame)\n",
    "                plt.axis('off')\n",
    "                plt.title(f'Test Episode - Reward: {reward:.2f}')\n",
    "                plt.show()\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        test_rewards.append(episode_reward)\n",
    "    avg_reward = np.mean(test_rewards)\n",
    "    print(f\"Average Test Reward over {num_episodes} episodes: {avg_reward:.2f}\")\n",
    "    return test_rewards\n",
    "\n",
    "def train_ppo(vec_envs, env, actor_critic, optimizer, num_episodes: int = 10000, max_frames: int = 30_000):\n",
    "    frame_idx = 0\n",
    "    test_rewards = []\n",
    "    episode_rewards = deque(maxlen=100)\n",
    "    \n",
    "    for episode in tqdm.tqdm(range(num_episodes)):\n",
    "        state, info = vec_envs.reset()\n",
    "        done = np.array([False] * vec_envs.num_envs)\n",
    "        truncated = np.array([False] * vec_envs.num_envs)\n",
    "        episode_reward = 0\n",
    "        states, actions, rewards, values, log_probs = [], [], [], [], []\n",
    "        \n",
    "        next_state = state  # Initialize next_state to current state\n",
    "\n",
    "        while (frame_idx < max_frames) and (not np.all(done)) and (not np.all(truncated)):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            #state_tensor.shape\n",
    "            #torch.Size([1, 2, 8])\n",
    "            action_probs, value = actor_critic(state_tensor)\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            next_state, reward, done, truncated, info = vec_envs.step(action.cpu().numpy()[0])\n",
    "            log_prob = dist.log_prob(action)\n",
    "            \n",
    "            states.append(state_tensor)\n",
    "            actions.append(action)\n",
    "            # convert reward to torch tensor\n",
    "            reward = torch.tensor(reward, dtype=torch.float32)\n",
    "            rewards.append(reward)\n",
    "            value = value.squeeze()\n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            frame_idx += 1\n",
    "            \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Only update if we have collected some data\n",
    "        if len(states) > 0:\n",
    "            # Compute GAE and returns\n",
    "            #torch.FloatTensor(next_state).unsqueeze(0).shape\n",
    "            #torch.Size([1, 2, 8])\n",
    "            _, next_value = actor_critic(torch.FloatTensor(next_state).unsqueeze(0))\n",
    "            next_value = next_value.squeeze()\n",
    "            returns = compute_gae(rewards, values, next_value)\n",
    "            \n",
    "            # Convert lists to tensors\n",
    "            states_tensor = torch.cat(states)\n",
    "            actions_tensor = torch.cat(actions)\n",
    "            log_probs_tensor = torch.cat(log_probs)\n",
    "            returns_tensor = torch.stack(returns)\n",
    "            \n",
    "            # Calculate advantages\n",
    "            values_tensor = torch.stack(values)\n",
    "            advantages = returns_tensor - values_tensor\n",
    "            \n",
    "            # Update the model using PPO\n",
    "            ppo_update(actor_critic, optimizer, states_tensor, actions_tensor, log_probs_tensor, returns_tensor, advantages)\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            test_reward = test_env(env, actor_critic, num_episodes=10, visualize=False)\n",
    "            test_rewards.append(np.mean(test_reward))\n",
    "            avg_reward = np.mean(episode_rewards) if episode_rewards else 0\n",
    "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}, Frame Index: {frame_idx}, Test Reward: {np.mean(test_reward):.2f}\")    \n",
    "    return test_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a53ad29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/30000 [01:27<728:19:30, 87.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Reward over 10 episodes: -157.31\n",
      "Episode 0, Average Reward: -12962.62, Frame Index: 6122, Test Reward: -157.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/30000 [01:33<390:05:08, 46.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train the PPO agent\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m test_rewards = \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_critic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m plot_rewards(test_rewards, title=\u001b[33m\"\u001b[39m\u001b[33mPPO Training Rewards\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mtrain_ppo\u001b[39m\u001b[34m(vec_envs, env, actor_critic, optimizer, num_episodes, max_frames)\u001b[39m\n\u001b[32m    102\u001b[39m     advantages = returns_tensor - values_tensor\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# Update the model using PPO\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[43mppo_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor_critic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturns_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvantages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m episode % \u001b[32m1000\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m    108\u001b[39m     test_reward = test_env(env, actor_critic, num_episodes=\u001b[32m10\u001b[39m, visualize=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mppo_update\u001b[39m\u001b[34m(actor_critic, optimizer, states, actions, old_log_probs, returns, advantages, clip_ratio)\u001b[39m\n\u001b[32m     39\u001b[39m loss = actor_loss + critic_loss\n\u001b[32m     41\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/dirtyRL/.venv/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/dirtyRL/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/dirtyRL/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train the PPO agent\n",
    "test_rewards = train_ppo(vec_envs, env, actor_critic, optimizer, num_episodes=30_000, max_frames=max_frames)\n",
    "plot_rewards(test_rewards, title=\"PPO Training Rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91e46c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: -126.10\n"
     ]
    }
   ],
   "source": [
    "# test the trained agent\n",
    "\n",
    "state, info = env.reset()\n",
    "\n",
    "frame = env.render()\n",
    "clear_output(wait=True)\n",
    "plt.imshow(frame)\n",
    "plt.axis('off')\n",
    "plt.title('Pendulum Environment')\n",
    "plt.show()\n",
    "done = False\n",
    "truncated = False\n",
    "total_reward = 0\n",
    "while not (done or truncated):\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    action_probs, value = actor_critic(state)\n",
    "    dist = Categorical(action_probs)\n",
    "    action = dist.sample()\n",
    "    next_state, reward, done, truncated, info = env.step(action.cpu().numpy()[0])\n",
    "    state = next_state\n",
    "    frame = env.render()\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    plt.title('Pendulum Environment')\n",
    "    plt.show()\n",
    "    total_reward += reward\n",
    "print(f\"Total Reward: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c2b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State tensor shape: torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "state, info = vec_envs.reset()\n",
    "episode_reward = 0\n",
    "states, actions, rewards, values, log_probs = [], [], [], [], []\n",
    "\n",
    "next_state = state  # Initialize next_state to current state\n",
    "state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "print(\"State tensor shape:\", state_tensor.shape)\n",
    "action_probs, value = actor_critic(state_tensor)def train_ppo(vec_envs, env, actor_critic, optimizer, num_episodes: int = 10000, max_frames: int = 30_000):\n",
    "    frame_idx = 0\n",
    "    test_rewards = []\n",
    "    episode_rewards = deque(maxlen=100)\n",
    "    \n",
    "    for episode in tqdm.tqdm(range(num_episodes)):\n",
    "        state, info = vec_envs.reset()\n",
    "        done = np.array([False, False])\n",
    "        truncated = np.array([False, False])\n",
    "        episode_reward = 0\n",
    "        states, actions, rewards, values, log_probs = [], [], [], [], []\n",
    "        \n",
    "        step_count = 0\n",
    "        max_steps_per_episode = 1000  # Prevent infinite episodes\n",
    "\n",
    "        while (frame_idx < max_frames) and (not np.all(done)) and (not np.all(truncated)) and (step_count < max_steps_per_episode):\n",
    "            # Don't add extra unsqueeze - state already has shape [2, 8]\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            action_probs, value = actor_critic(state_tensor)\n",
    "            \n",
    "            # Create distribution for each environment\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            # Use actions for both environments\n",
    "            next_state, reward, done, truncated, info = vec_envs.step(action.cpu().numpy())\n",
    "            log_prob = dist.log_prob(action)\n",
    "            \n",
    "            states.append(state_tensor)\n",
    "            actions.append(action)\n",
    "            \n",
    "            # Convert reward to torch tensor and sum across environments\n",
    "            reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
    "            rewards.append(reward_tensor.sum())  # Sum rewards across environments\n",
    "            \n",
    "            # Sum values and log_probs across environments\n",
    "            values.append(value.sum())\n",
    "            log_probs.append(log_prob.sum())\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward_tensor.sum()\n",
    "            frame_idx += 1\n",
    "            step_count += 1\n",
    "            \n",
    "        episode_rewards.append(episode_reward.item() if isinstance(episode_reward, torch.Tensor) else episode_reward)\n",
    "        \n",
    "        # Only update if we have collected some data\n",
    "        if len(states) > 0:\n",
    "            # Compute GAE and returns\n",
    "            state_tensor = torch.FloatTensor(next_state)\n",
    "            _, next_value = actor_critic(state_tensor)\n",
    "            next_value = next_value.sum()  # Sum across environments\n",
    "            \n",
    "            returns = compute_gae(rewards, values, next_value)\n",
    "            \n",
    "            # Convert lists to tensors\n",
    "            states_tensor = torch.stack(states)\n",
    "            actions_tensor = torch.stack(actions)\n",
    "            log_probs_tensor = torch.stack(log_probs)\n",
    "            returns_tensor = torch.stack(returns)\n",
    "            \n",
    "            # Calculate advantages\n",
    "            values_tensor = torch.stack(values)\n",
    "            advantages = returns_tensor - values_tensor\n",
    "            \n",
    "            # Normalize advantages for better training stability\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "            \n",
    "            # Update the model using PPO\n",
    "            ppo_update(actor_critic, optimizer, states_tensor, actions_tensor, log_probs_tensor, returns_tensor, advantages)\n",
    "\n",
    "        if episode % 100 == 0:  # Test more frequently\n",
    "            test_reward = test_env(env, actor_critic, num_episodes=5, visualize=False)\n",
    "            test_rewards.append(np.mean(test_reward))\n",
    "            avg_reward = np.mean(episode_rewards) if episode_rewards else 0\n",
    "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}, Frame Index: {frame_idx}, Test Reward: {np.mean(test_reward):.2f}\")    \n",
    "    return test_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1968af62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1961, 0.3169, 0.2212, 0.2657],\n",
       "         [0.1790, 0.3054, 0.2469, 0.2687],\n",
       "         [0.1613, 0.3620, 0.2369, 0.2398],\n",
       "         [0.1775, 0.3610, 0.2214, 0.2402]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "216f9425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Categorical(probs: torch.Size([1, 4, 4])), tensor([[1, 2, 2, 2]]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = Categorical(action_probs)\n",
    "action = dist.sample()\n",
    "dist, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0e648948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 2])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "705feaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7ed99598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_envs.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1760db16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 0])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_envs.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "325daf40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.0207015 ,  1.4376045 , -0.70805174,  0.36898848,  0.02823097,\n",
       "          0.22109969,  0.        ,  0.        ],\n",
       "        [ 0.0130722 ,  1.4391944 ,  0.4405354 ,  0.44750464, -0.01374587,\n",
       "         -0.09139954,  0.        ,  0.        ],\n",
       "        [ 0.01492996,  1.378388  ,  0.5005176 , -0.48031557, -0.02251425,\n",
       "         -0.17477141,  0.        ,  0.        ],\n",
       "        [-0.01559029,  1.4337385 , -0.7815169 ,  0.52460545,  0.01885197,\n",
       "          0.19445577,  0.        ,  0.        ]], dtype=float32),\n",
       " array([-1.49397397, -3.43256539,  1.92220309, -3.01464674]),\n",
       " array([False, False, False, False]),\n",
       " array([False, False, False, False]),\n",
       " {})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_envs.step(action.cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35605f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dirtyrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
